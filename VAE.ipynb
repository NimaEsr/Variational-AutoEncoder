{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xBkDXXAm_167"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "#from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcBY2JTMFakC",
    "outputId": "9a81e96e-b501-413f-86e0-555d834662b5"
   },
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "70vuPiH6Fdje"
   },
   "outputs": [],
   "source": [
    "#Norm.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Reshape \n",
    "img_width  = x_train.shape[1]\n",
    "img_height = x_train.shape[2]\n",
    "num_channels = 1 #MNIST --> grey scale so 1 channel\n",
    "x_train = x_train.reshape(x_train.shape[0], img_height, img_width, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_height, img_width, num_channels)\n",
    "input_shape = (img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5iuaKLeFhbJ",
    "outputId": "3859a6e5-a90e-49b7-eca4-d249c51c7a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 28, 28, 32)   320         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 14, 14, 64)   18496       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 12544)        0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           401440      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 2)            66          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " latent_sigma (Dense)           (None, 2)            66          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 2)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_sigma[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 494,244\n",
      "Trainable params: 494,244\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latent_dim = 2 # Number of latent dim parameters\n",
    "\n",
    "input_img = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x) #Shape of conv to be provided to decoder\n",
    "#Flatten\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "# Two outputs, for latent mean and log variance (std. dev.)\n",
    "#Use these to sample random variables in latent space to which inputs are mapped. \n",
    "z_mu = Dense(latent_dim, name='latent_mu')(x)   #Mean values of encoded input\n",
    "z_sigma = Dense(latent_dim, name='latent_sigma')(x)  #Std dev. (variance) of encoded input\n",
    "\n",
    "#REPARAMETERIZATION TRICK\n",
    "# Define sampling function to sample from the distribution\n",
    "# Reparameterize sample based on the process defined by Gunderson and Huang\n",
    "# into the shape of: mu + sigma squared x eps\n",
    "#This is to allow gradient descent to allow for gradient estimation accurately. \n",
    "def sample_z(args):\n",
    "  z_mu, z_sigma = args\n",
    "  eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
    "  return z_mu + K.exp(z_sigma / 2) * eps\n",
    "\n",
    "# sample vector from the latent distribution\n",
    "# z is the labda custom layer we are adding for gradient descent calculations\n",
    "  # using mu and variance (sigma)\n",
    "z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n",
    "\n",
    "#Z (lambda layer) will be the last layer in the encoder.\n",
    "# Define and summarize encoder model.\n",
    "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8bfZF0EFpKk",
    "outputId": "d08db52b-215a-45c8-853e-d7ab97ad8b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12544)             37632     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 28, 28, 32)       18464     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " decoder_output (Conv2DTrans  (None, 28, 28, 1)        289       \n",
      " pose)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,385\n",
      "Trainable params: 56,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder takes the latent vector as input\n",
    "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "\n",
    "# Need to start with a shape that can be remapped to original image shape as\n",
    "#we want our final utput to be same shape original input.\n",
    "#So, add dense layer with dimensions that can be reshaped to desired output shape\n",
    "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "# reshape to the shape of last conv. layer in the encoder, so we can \n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "# upscale (conv2D transpose) back to original shape\n",
    "# use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "#Can add more conv2DTranspose layers, if desired. \n",
    "#Using sigmoid activation\n",
    "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Define and summarize decoder model\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KXnHxSZ-FtId"
   },
   "outputs": [],
   "source": [
    "# apply the decoder to the latent sample \n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ArTrGI5cFuuH"
   },
   "outputs": [],
   "source": [
    "#Define custom loss\n",
    "#VAE is trained using two loss functions reconstruction loss and KL divergence\n",
    "#Let us add a class to define a custom layer with loss\n",
    "class CustomLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        # Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
    "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n",
    "        return K.mean(recon_loss + kl_loss)\n",
    "\n",
    "    # add custom loss to the class\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "# apply the custom loss to the input images and the decoded latent distribution sample\n",
    "y = CustomLayer()([input_img, z_decoded])\n",
    "# y is basically the original image after encoding input img to mu, sigma, z\n",
    "# and decoding sampled z values.\n",
    "#This will be used as output for vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vc9lrXemFxrX",
    "outputId": "4852a71a-f9a1-4f37-9586-0b4c26f64642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 28, 28, 32)   320         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 14, 14, 64)   18496       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 12544)        0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           401440      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 2)            66          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " latent_sigma (Dense)           (None, 2)            66          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 2)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_sigma[0][0]']           \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, 28, 28, 1)    56385       ['z[0][0]']                      \n",
      "                                                                                                  \n",
      " custom_layer (CustomLayer)     (None, 28, 28, 1)    0           ['encoder_input[0][0]',          \n",
      "                                                                  'decoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 550,629\n",
      "Trainable params: 550,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae = Model(input_img, y, name='vae')\n",
    "\n",
    "# Compile VAE\n",
    "vae.compile(optimizer='adam', loss=None)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "id": "YkOMIpzjF0mC",
    "outputId": "ec42d53c-d096-48c3-c56d-ab09e95d3205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 239, in __call__\n        self._loss_metric.update_state(\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 465, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\keras_tensor.py\", line 254, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train autoencoder\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefmoqjmd2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 239, in __call__\n        self._loss_metric.update_state(\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 465, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"C:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\keras_tensor.py\", line 254, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
     ]
    }
   ],
   "source": [
    "# Train autoencoder\n",
    "vae.fit(x_train, None, epochs = 10, batch_size = 32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw7KHwsqF200"
   },
   "outputs": [],
   "source": [
    "mu, _, _ = encoder.predict(x_test)\n",
    "#Plot dim1 and dim2 for mu\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(mu[:, 0], mu[:, 1], c=y_test, cmap='brg')\n",
    "plt.xlabel('dim 1')\n",
    "plt.ylabel('dim 2')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASZ7UqcKGBsI"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yMNK13RGCVC"
   },
   "outputs": [],
   "source": [
    "# Visualize images\n",
    "#Single decoded image with random input latent vector (of size 1x2)\n",
    "#Latent space range is about -5 to 5 so pick random values within this range\n",
    "#Try starting with -1, 1 and slowly go up to -1.5,1.5 and see how it morphs from \n",
    "#one image to the other.\n",
    "sample_vector = np.array([[1,-1]])\n",
    "decoded_example = decoder.predict(sample_vector)\n",
    "decoded_example_reshaped = decoded_example.reshape(img_width, img_height)\n",
    "plt.imshow(decoded_example_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lG4p04tiGFnk"
   },
   "outputs": [],
   "source": [
    "n = 20  # generate 15x15 digits\n",
    "figure = np.zeros((img_width * n, img_height * n, num_channels))\n",
    "\n",
    "#Create a Grid of latent variables, to be provided as inputs to decoder.predict\n",
    "#Creating vectors within range -5 to 5 as that seems to be the range in latent space\n",
    "grid_x = np.linspace(-5, 5, n)\n",
    "grid_y = np.linspace(-5, 5, n)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aREvui5GGbq"
   },
   "outputs": [],
   "source": [
    "# decoder for each square in the grid\n",
    "for i, yi in enumerate(grid_y):\n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
    "        figure[i * img_width: (i + 1) * img_width,\n",
    "               j * img_height: (j + 1) * img_height] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "#Reshape for visualization\n",
    "fig_shape = np.shape(figure)\n",
    "figure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
    "\n",
    "plt.imshow(figure, cmap='gnuplot2')\n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
